import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
from sklearn.model_selection import train_test_split
from sklearn.datasets import fetch_openml  # 用于获取数据集，这里以MNIST为例的替代
from sklearn.preprocessing import LabelEncoder
import numpy as np

# 假设我们已经有了一些标记和未标记的数据
# 这里我们使用fetch_openml获取MNIST数据集作为示例
# 注意：实际使用时，需要根据论文中的具体设置来处理数据集
mnist = fetch_openml('mnist_784', version=1)
X = mnist.data.astype(np.float32) / 255.0  # 归一化
y = mnist.target.astype(np.int64)

# 模拟有标签和无标签数据（论文中的设置）
labeled_indices = np.random.choice(X.shape[0], size=1000, replace=False)  # 选择1000个有标签样本
X_labeled, y_labeled = X[labeled_indices], y[labeled_indices]
X_unlabeled = X[~np.isin(np.arange(X.shape[0]), labeled_indices)]  # 剩余为无标签样本

# 划分训练集和测试集（这里简单划分，论文中可能有更复杂的划分方式）
X_train_labeled, X_val_labeled, y_train_labeled, y_val_labeled = train_test_split(X_labeled, y_labeled, test_size=0.2, random_state=42)

# 转换为PyTorch张量
train_labeled_dataset = TensorDataset(torch.tensor(X_train_labeled), torch.tensor(y_train_labeled))
val_labeled_dataset = TensorDataset(torch.tensor(X_val_labeled), torch.tensor(y_val_labeled))
train_unlabeled_dataset = TensorDataset(torch.tensor(X_unlabeled))

train_loader_labeled = DataLoader(train_labeled_dataset, batch_size=64, shuffle=True)
val_loader_labeled = DataLoader(val_labeled_dataset, batch_size=64, shuffle=False)
train_loader_unlabeled = DataLoader(train_unlabeled_dataset, batch_size=64, shuffle=True)

# 定义一个简单的神经网络模型
class SimpleNN(nn.Module):
    def __init__(self):
        super(SimpleNN, self).__init__()
        self.fc1 = nn.Linear(784, 500)
        self.relu = nn.ReLU()
        self.fc2 = nn.Linear(500, 10)  # MNIST有10个类别

    def forward(self, x):
        x = x.view(-1, 784)  # 展平输入
        x = self.fc1(x)
        x = self.relu(x)
        x = self.fc2(x)
        return x

# 初始化模型、损失函数和优化器
model = SimpleNN()
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

# 训练模型（伪标签方法）
num_epochs = 10
for epoch in range(num_epochs):
    model.train()
    for labeled_batch, unlabeled_batch in zip(train_loader_labeled, train_loader_unlabeled):
        labeled_inputs, labeled_targets = labeled_batch
        unlabeled_inputs = unlabeled_batch[0]

        # 1. 使用有标签数据进行监督训练
        optimizer.zero_grad()
        labeled_outputs = model(labeled_inputs)
        labeled_loss = criterion(labeled_outputs, labeled_targets)
        labeled_loss.backward()
        optimizer.step()

        # 2. 为无标签数据生成伪标签（选择预测概率最大的类别作为伪标签）
        with torch.no_grad():
            unlabeled_outputs = model(unlabeled_inputs)
            pseudo_labels = unlabeled_outputs.argmax(dim=1)

        # 3. 使用伪标签和无标签数据进行半监督训练（这里可以添加权重平衡有标签和无标签损失）
        # 注意：实际论文中可能有更复杂的损失函数和处理方式
        optimizer.zero_grad()
        unlabeled_loss = criterion(unlabeled_outputs, pseudo_labels.detach())  # 使用detach避免梯度计算到伪标签上
        # 合并损失（这里简单相加，论文中可能有更复杂的权重和调度策略）
        combined_loss = labeled_loss + unlabeled_loss * 0.1  # 假设给无标签损失一个较小的权重
        combined_loss.backward()
        optimizer.step()

    # 验证模型性能（仅使用有标签验证集）
    model.eval()
    val_loss = 0.0
    correct = 0
    with torch.no_grad():
        for val_inputs, val_targets in val_loader_labeled:
            val_outputs = model(val_inputs)
            loss = criterion(val_outputs, val_targets)
            val_loss += loss.item()
            _, predicted = torch.max(val_outputs, 1)
            correct += (predicted == val_targets).sum().item()

    val_loss /= len(val_loader_labeled)
    accuracy = correct / len(val_loader_labeled.dataset)
    print(f'Epoch [{epoch+1}/{num_epochs}], Validation Loss: {val_loss:.4f}, Accuracy: {accuracy:.4f}')
